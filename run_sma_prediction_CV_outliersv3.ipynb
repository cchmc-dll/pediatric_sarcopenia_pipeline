{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load libraries and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from IPython import get_ipython\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "\n",
    "# Custom modules for debugging\n",
    "from SliceViewer import ImageSliceViewer3D, ImageSliceViewer3D_1view,ImageSliceViewer3D_2views\n",
    "from investigate import *\n",
    "\n",
    "#pd.set_option(\"display.max_rows\", 10)\n",
    "      \n",
    "import json\n",
    "from run_sma_experiment import find_l3_images,output_images\n",
    "import pprint\n",
    "from L3_finder import *\n",
    "\n",
    "# Custom functions\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):        \n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_line_magic('tb', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/smipipeline\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "data = '/tf/data'\n",
    "pickles = '/tf/pickles'\n",
    "models = '/tf/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l3_finder': {'cache_dir': '/tf/_cache/',\n",
      "               'cache_intermediate_results': True,\n",
      "               'dicom_dir': '/tf/data',\n",
      "               'model_path_dir': '/tf/models/l3/cv_final',\n",
      "               'new_tim_dicom_dir_structure': True,\n",
      "               'output_directory': '/tf/output/cv_poorl3/l3',\n",
      "               'overwrite': True,\n",
      "               'save_plots': True,\n",
      "               'show_plots': False},\n",
      " 'muscle_segmentor': {'model_path_dir': '/tf/models/muscle/cv_final',\n",
      "                      'output_directory': '/tf/output/cv_poorl3/ms'}}\n"
     ]
    }
   ],
   "source": [
    "# Import modules and config file\n",
    "configfile = os.path.join(cwd,'config/debug_ES/run_prediction_CV_poorl3.json')\n",
    "with open(configfile, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Process final images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Load each study into subject object\n",
    "<br>\n",
    "Subject object defined in L3finder.ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['l3_finder']['new_tim_dicom_dir_structure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding subjects\n",
      "Subjects found:  2367\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "print(\"Finding subjects\")\n",
    "\n",
    "subjects = list(\n",
    "    find_subjects(\n",
    "        config['l3_finder'][\"dicom_dir\"],\n",
    "        new_tim_dir_structure=config['l3_finder']['new_tim_dicom_dir_structure']\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Subjects found: ', len(subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section-3 - check if there are subjects with multiple folders (studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding series\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4473070ff14830a68cd505f118a708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2367.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 290 ms, sys: 149 ms, total: 438 ms\n",
      "Wall time: 682 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Find series images\n",
    "print(\"Finding series\")\n",
    "series = list(flatten(tqdm((s.find_series() for s in subjects),total=len(subjects))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of series found:  3758\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of series found: \", len(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering series\n",
      "CPU times: user 858 ms, sys: 836 ms, total: 1.69 s\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sagittal_series, axial_series, excluded_series = separate_series(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of valid pats:  2367\n",
      "Length of sagittal series 1391\n",
      "Length of axial series 2367\n",
      "Length of excluded series 0\n",
      "Length of all series in dataset 3758\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of valid pats: \", len(subjects))\n",
    "print(\"Length of sagittal series\", len(sagittal_series))\n",
    "print(\"Length of axial series\", len(axial_series))\n",
    "print(\"Length of excluded series\", len(excluded_series))\n",
    "print(\"Length of all series in dataset\", len(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure each subject has at the max only 1 axial and 1 sagittal series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_ids = [ax.subject.id_ for ax in axial_series]\n",
    "sag_ids = [sag.subject.id_ for sag in sagittal_series]\n",
    "\n",
    "def find_duplicates(id_list):\n",
    "    uniques = []\n",
    "    duplicates = []\n",
    "    for ids in id_list:\n",
    "        if ids in uniques:\n",
    "            duplicates.append(ids)\n",
    "        else:\n",
    "            uniques.append(ids)\n",
    "            \n",
    "    return uniques,duplicates\n",
    "\n",
    "\n",
    "ax_u,ax_d = find_duplicates(ax_ids)\n",
    "sag_u,sag_d = find_duplicates(sag_ids)\n",
    "\n",
    "print('Ax duplicates: ', ax_d)\n",
    "print('Sag duplicates: ', sag_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the series objects to investigate\n",
    "ax_d_series = [ax for ax in axial_series if ax.subject.id_ in ax_d]\n",
    "sag_d_series = [ax for ax in sagittal_series if ax.subject.id_ in ax_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dl= load_object(os.path.join(pickles,'df_final.pkl'))\n",
    "display(df_dl[df_dl['ID']==ax_d[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct Missing Sagittals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default code filters 0.5mm slices, but I am letting them pass by setting it to 0\n",
    "constructed_sagittals = construct_series_for_subjects_without_sagittals(\n",
    "        subjects, sagittal_series, axial_series,thickness_filter=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "        \"Series separated\\n\",\n",
    "        len(sagittal_series), \"sagittal series.\",\n",
    "        len(axial_series), \"axial series.\",\n",
    "        len(excluded_series), \"excluded series.\",\n",
    "        len(constructed_sagittals), \"constructed series.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagittal_series.extend(constructed_sagittals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(axial_series,os.path.join(pickles,'axial_curated.pkl'))\n",
    "save_object(sagittal_series,os.path.join(pickles,'sagittal_curated.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating sagittal MIPS\")\n",
    "mips = create_sagittal_mips_from_series(\n",
    "        many_series=sagittal_series,\n",
    "        cache_dir=config['l3_finder'].get(\"cache_dir\", None),\n",
    "        cache=config['l3_finder'].get(\"cache_intermediate_results\", False),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(mips,os.path.join(pickles,'mips.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mips = load_object(os.path.join(pickles,'mips.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing Images\")\n",
    "preprocessed_images = preprocess_images(mips)\n",
    "\n",
    "# Sagittal mip is redundant, get rid just use preprocessed images\n",
    "sagittal_mips = [SagittalMIP(i) for i in preprocessed_images]\n",
    "\n",
    "print(\"Separate heights for better batching\")\n",
    "mips_by_dimension = group_mips_by_dimension(sagittal_mips)\n",
    "print(\"Dimensions in set:\", mips_by_dimension.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(mips_by_dimension,os.path.join(pickles,'mips_by_dimension.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find L3 - step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mips_by_dimension = load_object(os.path.join(pickles,'mips_by_dimension.pkl'))\n",
    "axial_series = load_object(os.path.join(pickles,'axial_curated.pkl'))\n",
    "sagittal_series = load_object(os.path.join(pickles,'sagittal_curated.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNet1D_cv_1_of_5.h5', 'UNet1D_cv_2_of_5.h5', 'UNet1D_cv_3_of_5.h5', 'UNet1D_cv_4_of_5.h5', 'UNet1D_cv_5_of_5.h5']\n",
      "/tf/models/l3/cv_final/UNet1D_cv_1_of_5.h5\n",
      "/tf/models/l3/cv_final/UNet1D_cv_2_of_5.h5\n",
      "/tf/models/l3/cv_final/UNet1D_cv_3_of_5.h5\n",
      "/tf/models/l3/cv_final/UNet1D_cv_4_of_5.h5\n",
      "/tf/models/l3/cv_final/UNet1D_cv_5_of_5.h5\n"
     ]
    }
   ],
   "source": [
    "# Get all models in model path dir\n",
    "models_dir = config['l3_finder']['model_path_dir']\n",
    "\n",
    "# Get all models in models dir\n",
    "models_list = sorted([f for f in os.listdir(models_dir) if f.endswith('.h5')])\n",
    "print(models_list)\n",
    "folds = len(models_list)\n",
    "\n",
    "for fold in range(folds):\n",
    "    model_path = os.path.join(models_dir,models_list[fold])\n",
    "    print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for fold  0 Path:  /tf/models/l3/cv_final/UNet1D_cv_1_of_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error drawing line on preprocessed_image for: Z1996094\n",
      "shape: (125, 512, 1), prediction: 126\n",
      "\n",
      "error drawing line on preprocessed_image for: Z701477\n",
      "shape: (185, 512, 1), prediction: 186\n",
      "\n",
      "error drawing line on preprocessed_image for: Z495526\n",
      "shape: (215, 512, 1), prediction: 216\n",
      "\n",
      "error drawing line on preprocessed_image for: Z362374\n",
      "shape: (888, 512, 1), prediction: 888\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for fold  1 Path:  /tf/models/l3/cv_final/UNet1D_cv_2_of_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error drawing line on preprocessed_image for: Z892856\n",
      "shape: (430, 512, 1), prediction: 431\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1996094\n",
      "shape: (125, 512, 1), prediction: 127\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1140510\n",
      "shape: (245, 512, 1), prediction: 246\n",
      "\n",
      "error drawing line on preprocessed_image for: Z495526\n",
      "shape: (215, 512, 1), prediction: 216\n",
      "\n",
      "error drawing line on preprocessed_image for: Z878416\n",
      "shape: (462, 512, 1), prediction: 463\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1196674\n",
      "shape: (410, 512, 1), prediction: 411\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1273303\n",
      "shape: (495, 512, 1), prediction: 496\n",
      "\n",
      "error drawing line on preprocessed_image for: Z391986\n",
      "shape: (420, 512, 1), prediction: 421\n",
      "\n",
      "error drawing line on preprocessed_image for: Z526993\n",
      "shape: (282, 512, 1), prediction: 283\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1239639\n",
      "shape: (360, 512, 1), prediction: 361\n",
      "\n",
      "error drawing line on preprocessed_image for: Z913179\n",
      "shape: (220, 512, 1), prediction: 221\n",
      "\n",
      "error drawing line on preprocessed_image for: Z331988\n",
      "shape: (435, 512, 1), prediction: 437\n",
      "\n",
      "error drawing line on preprocessed_image for: Z854818\n",
      "shape: (425, 512, 1), prediction: 426\n",
      "\n",
      "error drawing line on preprocessed_image for: Z15992\n",
      "shape: (365, 512, 1), prediction: 367\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1293823\n",
      "shape: (185, 512, 1), prediction: 186\n",
      "\n",
      "error drawing line on preprocessed_image for: Z857466\n",
      "shape: (410, 512, 1), prediction: 411\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1302641\n",
      "shape: (752, 512, 1), prediction: 753\n",
      "\n",
      "error drawing line on preprocessed_image for: Z511402\n",
      "shape: (540, 512, 1), prediction: 541\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1211258\n",
      "shape: (576, 512, 1), prediction: 577\n",
      "\n",
      "error drawing line on preprocessed_image for: Z486791\n",
      "shape: (650, 512, 1), prediction: 651\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for fold  2 Path:  /tf/models/l3/cv_final/UNet1D_cv_3_of_5.h5\n",
      "Making predictions for fold  3 Path:  /tf/models/l3/cv_final/UNet1D_cv_4_of_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error drawing line on preprocessed_image for: Z495526\n",
      "shape: (215, 512, 1), prediction: 216\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for fold  4 Path:  /tf/models/l3/cv_final/UNet1D_cv_5_of_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error drawing line on preprocessed_image for: Z627604\n",
      "shape: (455, 512, 1), prediction: 458\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1996094\n",
      "shape: (125, 512, 1), prediction: 127\n",
      "\n",
      "error drawing line on preprocessed_image for: Z495526\n",
      "shape: (215, 512, 1), prediction: 217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runname = 'CV_poorl3'\n",
    "if __name__ == \"__main__\":\n",
    "    for fold in range(folds):\n",
    "        model_path = os.path.join(models_dir,models_list[fold])\n",
    "        print(\"Making predictions for fold \", fold, 'Path: ', model_path)\n",
    "        prediction_results = []\n",
    "        prediction_errors = []\n",
    "        for dimension, sagittal_mips in mips_by_dimension.items():\n",
    "            dim_group_results,errors = make_predictions_for_sagittal_mips(\n",
    "                sagittal_mips,\n",
    "                model_path=model_path,\n",
    "                shape=dimension\n",
    "            )\n",
    "            prediction_results.extend(dim_group_results)\n",
    "            prediction_errors.extend(errors)\n",
    "\n",
    "        # Save prediction results\n",
    "        pred_results_file = 'prediction_results_' + str(fold) + '_' +  runname + '.pkl'\n",
    "        pred_errors_file = 'prediction_errors_' + str(fold) + '_' +  runname + '.pkl'\n",
    "        save_object(prediction_results,os.path.join(pickles,pred_results_file))\n",
    "        save_object(prediction_errors,os.path.join(pickles,pred_errors_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save l3 prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_series = load_object(os.path.join(pickles,'axial_curated.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2367"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(axial_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Total predictions:  2363\n",
      "Building L3 images for fold:  0\n",
      "Total images:  2363\n",
      "Outputting L3 images for fold:  0\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 923/2363 [02:32<19:08,  1.25it/s]  /tf/smipipeline/l3finder/output.py:142: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  assert img_max + (-img_min) < 65536\n",
      "100%|██████████| 2363/2363 [13:16<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions:  2347\n",
      "Building L3 images for fold:  1\n",
      "Total images:  2347\n",
      "Outputting L3 images for fold:  1\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2347/2347 [13:10<00:00,  2.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions:  2367\n",
      "Building L3 images for fold:  2\n",
      "Total images:  2367\n",
      "Outputting L3 images for fold:  2\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1366/2367 [03:42<03:47,  4.39it/s] /tf/smipipeline/l3finder/output.py:142: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  assert img_max + (-img_min) < 65536\n",
      "100%|██████████| 2367/2367 [13:25<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions:  2366\n",
      "Building L3 images for fold:  3\n",
      "Total images:  2366\n",
      "Outputting L3 images for fold:  3\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 1401/2366 [03:47<07:05,  2.27it/s] /tf/smipipeline/l3finder/output.py:142: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  assert img_max + (-img_min) < 65536\n",
      " 96%|█████████▌| 2274/2366 [09:17<00:46,  1.97it/s]/tf/smipipeline/l3finder/output.py:142: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  assert img_max + (-img_min) < 65536\n",
      "100%|██████████| 2366/2366 [13:26<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions:  2364\n",
      "Building L3 images for fold:  4\n",
      "Total images:  2364\n",
      "Outputting L3 images for fold:  4\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2364/2364 [13:12<00:00,  2.98it/s] \n"
     ]
    }
   ],
   "source": [
    "# Load prediction_results pickle files\n",
    "runname = 'CV_poorl3'\n",
    "\n",
    "output_dir = config[\"l3_finder\"][\"output_directory\"]\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "prediction_results_list = sorted([f for f in os.listdir(pickles) if (f.startswith('prediction_results_') and f.endswith(runname+'.pkl'))])\n",
    "folds = len(prediction_results_list)\n",
    "print(folds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold in range(folds):\n",
    "        pred_file = os.path.join(pickles,prediction_results_list[fold])\n",
    "        prediction_results = load_object(pred_file)\n",
    "        print('Total predictions: ',len(prediction_results))\n",
    "        print('Building L3 images for fold: ', fold)\n",
    "        l3_images = build_l3_images(axial_series, prediction_results)\n",
    "        print('Total images: ',len(l3_images))\n",
    "        # Don't run this unless you have new L3 results\n",
    "        print(\"Outputting L3 images for fold: \", fold)\n",
    "        # Clears pixel data from memory aafter outputting\n",
    "        output_dir = os.path.join(config[\"l3_finder\"][\"output_directory\"],str(fold))\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        l3_images = output_images(\n",
    "        l3_images,\n",
    "        args=dict(\n",
    "            output_directory=output_dir,\n",
    "            should_plot=config[\"l3_finder\"][\"show_plots\"],\n",
    "            should_overwrite=config[\"l3_finder\"][\"overwrite\"],\n",
    "            should_save_plots=config[\"l3_finder\"][\"save_plots\"]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find mean L3 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "from L3_finder import L3Image\n",
    "from l3finder import ingest\n",
    "\n",
    "from compare_best_to_manual_l3_and_seg import MinimalPrediction, MinimalResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_l3_predictions(l3_prediction_dir,nfolds):\n",
    "    subject_id_col = 0\n",
    "    pred_in_px_col = 1\n",
    "    predictions = defaultdict(list)\n",
    "\n",
    "    for fold_index in range(0, nfolds):\n",
    "        csv_dir = os.path.join(l3_prediction_dir,str(fold_index))\n",
    "        csv_path = Path(csv_dir,'l3_prediction_results.csv')\n",
    "        with open(csv_path) as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader)\n",
    "\n",
    "            for row in reader:\n",
    "                subid = row[subject_id_col].split('-')[0]\n",
    "                predictions[subid].append(float(row[pred_in_px_col]))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def calc_mean_predictions(all_predictions: defaultdict):\n",
    "    result = {}\n",
    "    for subject_id, prediction_list in all_predictions.items():\n",
    "        result[subject_id] = np.mean(prediction_list)\n",
    "    return result\n",
    "\n",
    "def find_subjects_w_preds(predictions, all_subjects):\n",
    "    subject_ids_w_preds = set(predictions.keys())\n",
    "    return [s for s in all_subjects if s.id_ in subject_ids_w_preds]\n",
    "\n",
    "def load_l3_images_from_predictions(mean_predictions, subjects_w_preds,axials,sagittals):\n",
    "    l3_images = []\n",
    "\n",
    "    for subject in subjects_w_preds:\n",
    "        sagittal_series = [s for s in sagittals if s.subject.id_ == subject.id_][0]\n",
    "        axial_series = [a for a in axials if a.subject.id_ == subject.id_][0]\n",
    "        l3_images.append(\n",
    "            L3Image(\n",
    "                axial_series=axial_series,\n",
    "                sagittal_series=sagittal_series,\n",
    "                prediction_result=MinimalResult(\n",
    "                    MinimalPrediction(\n",
    "                        predicted_y_in_px=mean_predictions[subject.id_]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    return l3_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_series = load_object(os.path.join(pickles,'axial_curated.pkl'))\n",
    "sagittal_series = load_object(os.path.join(pickles,'sagittal_curated.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "runname = 'CV_poorl3'\n",
    "prediction_results_list = sorted([f for f in os.listdir(pickles) if (f.startswith('prediction_results_') and f.endswith(runname+'.pkl'))])\n",
    "folds = len(prediction_results_list)\n",
    "print(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    all_predictions = load_l3_predictions(config[\"l3_finder\"][\"output_directory\"],folds)\n",
    "    mean_predictions = calc_mean_predictions(all_predictions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    subjects_w_preds = find_subjects_w_preds(mean_predictions, list(ingest.find_subjects(config['l3_finder'][\"dicom_dir\"])))\n",
    "    l3_images = load_l3_images_from_predictions(mean_predictions, subjects_w_preds, axial_series, sagittal_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(l3_images,os.path.join(pickles,'l3_images_cv.pkl'))\n",
    "save_object(mean_predictions,os.path.join(pickles,'mean_predictions.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Outlier Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_images = load_object(os.path.join(pickles,'l3_images_cv.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outliers for manual L3 detection:  45\n",
      "Cases with L3 not present:  15\n",
      "Cases with manually identified L3s:  30\n"
     ]
    }
   ],
   "source": [
    "infile  = 'poorl3.csv'\n",
    "df_poorl3 = pd.read_csv(infile, index_col=False)\n",
    "\n",
    "print('Total number of outliers for manual L3 detection: ', len(df_poorl3))\n",
    "l3_absent = df_poorl3.loc[df_poorl3['L3slice'].isnull(),'ID'].values.tolist()\n",
    "print('Cases with L3 not present: ', len(l3_absent))\n",
    "l3_present = df_poorl3.loc[~df_poorl3['L3slice'].isnull(),'ID'].values.tolist()\n",
    "print('Cases with manually identified L3s: ', len(l3_present))\n",
    "\n",
    "#sagittal_mips_valid = [sagittal_mip for sagittal_mip in sagittal_mips if sagittal_mip.subject_id not in df_poorl3.ID.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>L3slice</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z1996094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this appears to be just additional images thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z1262030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a lung MIP and chest only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z705523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a lung MIP and chest only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z1762020</td>\n",
       "      <td>99.0</td>\n",
       "      <td>this is a lung MIP - is there not a soft tissu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z1195730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a lung MIP and chest only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Z1286046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a lung MIP and chest only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Z418856</td>\n",
       "      <td>101.0</td>\n",
       "      <td>this is a lung MIP - is there not a soft tissu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Z1450371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a lung MIP and chest only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Z687529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a lung MIP and chest only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Z1436778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a lung MIP and chest only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  L3slice                                            comment\n",
       "0  Z1996094      NaN  this appears to be just additional images thro...\n",
       "1  Z1262030      NaN                  this is a lung MIP and chest only\n",
       "2   Z705523      NaN                  this is a lung MIP and chest only\n",
       "3  Z1762020     99.0  this is a lung MIP - is there not a soft tissu...\n",
       "4  Z1195730      NaN                  this is a lung MIP and chest only\n",
       "5  Z1286046      NaN                  this is a lung MIP and chest only\n",
       "6   Z418856    101.0  this is a lung MIP - is there not a soft tissu...\n",
       "7  Z1450371      NaN                  this is a lung MIP and chest only\n",
       "8   Z687529      NaN                  this is a lung MIP and chest only\n",
       "9  Z1436778      NaN                  this is a lung MIP and chest only"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_poorl3.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total l3_images:  2367\n",
      "Total l3_images after outlier removal:  2352\n"
     ]
    }
   ],
   "source": [
    "# Get rid of outliers without proper L3 images\n",
    "print('Total l3_images: ', len(l3_images))\n",
    "l3_images = [l3_image for l3_image in l3_images if l3_image.subject_id not in l3_absent]\n",
    "print('Total l3_images after outlier removal: ', len(l3_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "l3_images_out = [l3_image for l3_image in l3_images if l3_image.subject_id in l3_present]\n",
    "print(len(l3_images_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Z1762020': 1857.4905660377358, 'Z717870': 923.4038461538462, 'Z362374': -316.2702702702703, 'Z890238': -1221.0151515151515, 'Z1722108': -1435.9842767295597, 'Z1256768': 1287.5316455696202, 'Z1119985': -236.65, 'Z418856': -1391.9749774932861, 'Z678707': 1446.3648648648648, 'Z670302': 1336.5046728971963, 'Z1302641': -172.61810952044547, 'Z489517': 1716.9052287581699, 'Z320930': 1914.5, 'Z1263347': 1710.5, 'Z1000800': 1521.5645161290322, 'Z1274627': -127.5995145631068, 'Z5745': 1810.8378305970245, 'Z1029886': 1574.323943661972, 'Z832424': 1411.3809523809523, 'Z441830': 1695.107142857143, 'Z1041413': 1744.8521126760563, 'Z567376': 1971.142857142857, 'Z1332420': 1643.7625899280574, 'Z1221549': 1329.0418648905804, 'Z511402': -423.79398148148147, 'Z357478': 1580.855828220859, 'Z1211258': -310.29166666666663, 'Z486791': -455.30769230769226, 'Z837620': 1739.2115331607183, 'Z627309': -215.90540540540542}\n"
     ]
    }
   ],
   "source": [
    "# Create Manual Predictions DICT\n",
    "manual_predictions = {}\n",
    "for i in range(len(l3_images_out)):\n",
    "    subject_id = l3_images_out[i].subject_id\n",
    "    start = l3_images_out[i].prediction_index[1].first_axial_pos\n",
    "    end = l3_images_out[i].prediction_index[1].last_axial_pos\n",
    "    counts = l3_images_out[i].prediction_index[1].axial_image_count\n",
    "    z_pos = l3_images_out[i].prediction_index[1].predicted_z_position\n",
    "    idx = l3_images_out[i].prediction_index[1].l3_axial_image_index\n",
    "\n",
    "    new_idx = df_poorl3.loc[df_poorl3['ID']==subject_id,'L3slice'].values[0]\n",
    "#     print('subject_id: ', subject_id, 'new_idx: = ',new_idx)    \n",
    "#     break\n",
    "\n",
    "    thick = abs((start-end)/counts)\n",
    "    \n",
    "    new_z_pos = 0\n",
    "    if start > end:\n",
    "        new_z_pos = start - thick*new_idx\n",
    "    else:\n",
    "        new_z_pos = start + thick*new_idx\n",
    "    \n",
    "    manual_predictions[subject_id] = new_z_pos\n",
    "    \n",
    "#     if new_z_pos < 0:\n",
    "#         print('thick: ', thick)\n",
    "#         print('start: ', start)\n",
    "#         print('end: ', end)\n",
    "#         print('old idx: ', idx)\n",
    "#         print('old z: ', z_pos)\n",
    "      \n",
    "#         print('new idx: ', new_idx)\n",
    "#         print('new z: ', new_z_pos)\n",
    "    \n",
    "#     print('\\n')\n",
    "    \n",
    "print(manual_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Old prediction:  821.8  new prediction:  1857.4905660377358\n",
      " Old prediction:  207.6  new prediction:  923.4038461538462\n",
      " Old prediction:  528.0  new prediction:  -316.2702702702703\n",
      " Old prediction:  495.6  new prediction:  -1221.0151515151515\n",
      " Old prediction:  690.2  new prediction:  -1435.9842767295597\n",
      " Old prediction:  168.2  new prediction:  1287.5316455696202\n",
      " Old prediction:  226.6  new prediction:  -236.65\n",
      " Old prediction:  736.8  new prediction:  -1391.9749774932861\n",
      " Old prediction:  107.2  new prediction:  1446.3648648648648\n",
      " Old prediction:  554.0  new prediction:  1336.5046728971963\n",
      " Old prediction:  348.75  new prediction:  -172.61810952044547\n",
      " Old prediction:  459.0  new prediction:  1716.9052287581699\n",
      " Old prediction:  155.6  new prediction:  1914.5\n",
      " Old prediction:  95.8  new prediction:  1710.5\n",
      " Old prediction:  322.6  new prediction:  1521.5645161290322\n",
      " Old prediction:  377.4  new prediction:  -127.5995145631068\n",
      " Old prediction:  427.4  new prediction:  1810.8378305970245\n",
      " Old prediction:  123.6  new prediction:  1574.323943661972\n",
      " Old prediction:  156.2  new prediction:  1411.3809523809523\n",
      " Old prediction:  275.2  new prediction:  1695.107142857143\n",
      " Old prediction:  357.0  new prediction:  1744.8521126760563\n",
      " Old prediction:  421.2  new prediction:  1971.142857142857\n",
      " Old prediction:  499.0  new prediction:  1643.7625899280574\n",
      " Old prediction:  246.2  new prediction:  1329.0418648905804\n",
      " Old prediction:  471.5  new prediction:  -423.79398148148147\n",
      " Old prediction:  662.0  new prediction:  1580.855828220859\n",
      " Old prediction:  246.0  new prediction:  -310.29166666666663\n",
      " Old prediction:  267.25  new prediction:  -455.30769230769226\n",
      " Old prediction:  276.8  new prediction:  1739.2115331607183\n",
      " Old prediction:  212.8  new prediction:  -215.90540540540542\n"
     ]
    }
   ],
   "source": [
    "mean_predictions_old = mean_predictions.copy()\n",
    "for key,value in manual_predictions.items():\n",
    "    print(' Old prediction: ', mean_predictions[key],' new prediction: ', manual_predictions[key])\n",
    "    mean_predictions[key] = round(manual_predictions[key],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_images = load_l3_images_from_predictions(mean_predictions, subjects_w_preds, axial_series, sagittal_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total l3_images:  2367\n",
      "Total l3_images after outlier removal:  2352\n"
     ]
    }
   ],
   "source": [
    "# Get rid of outliers without proper L3 images\n",
    "print('Total l3_images: ', len(l3_images))\n",
    "l3_images = [l3_image for l3_image in l3_images if l3_image.subject_id not in l3_absent]\n",
    "print('Total l3_images after outlier removal: ', len(l3_images))\n",
    "save_object(l3_images,os.path.join(pickles,'l3_images_cv_outliershandled.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify manual L3 selection was successfuly by comparing slice location in image to manual selection csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_images_manual = [l3_image for l3_image in l3_images if l3_image.subject_id in l3_present]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subject ID   Slice Z position   Slice Index   Manual Index\n",
      "Z1762020       1429.0        158       99.0\n",
      "Z717870       763.0        42       30.0\n",
      "Z362374       -682.0        146       55.0\n",
      "Z890238       -1367.0        112       79.0\n",
      "Z1722108       -1729.0        158       100.0\n",
      "Z1256768       1317.0        34       40.0\n",
      "Z1119985       -376.0        79       42.0\n",
      "Z418856       -1727.0        159       101.0\n",
      "Z678707       1514.0        21       36.0\n",
      "Z670302       1048.0        106       67.0\n",
      "Z1302641       -375.0        348       147.0\n",
      "Z489517       1624.0        115       92.0\n",
      "Z320930       1914.0        0       33.0\n",
      "Z1263347       1710.0        0       37.0\n",
      "Z1000800       1276.0        65       36.0\n",
      "Z1274627       -376.0        74       38.0\n",
      "Z5745       1440.0        1300       491.0\n",
      "Z1029886       1614.0        25       33.0\n",
      "Z832424       1453.0        31       40.0\n",
      "Z441830       1471.0        55       30.0\n",
      "Z1041413       1462.0        212       75.0\n",
      "Z567376       1703.0        130       44.0\n",
      "Z1332420       1486.0        113       77.0\n",
      "Z1221549       1232.0        821       497.0\n",
      "Z511402       -686.0        214       169.0\n",
      "Z357478       1300.0        162       96.0\n",
      "Z1211258       -384.0        82       58.0\n",
      "Z486791       -547.0        78       53.0\n",
      "Z837620       1650.0        799       525.0\n",
      "Z627309       -276.0        43       31.0\n"
     ]
    }
   ],
   "source": [
    "print(' Subject ID ',' Slice Z position ',' Slice Index','  Manual Index')\n",
    "for i in range(len(l3_images_out)):\n",
    "    subject_id = l3_images_out[i].subject_id\n",
    "    z_pos = l3_images_out[i].prediction_index[1].predicted_z_position\n",
    "    idx = l3_images_out[i].prediction_index[1].l3_axial_image_index\n",
    "    print(subject_id,'     ', np.round(z_pos,0),'      ', idx, '     ',df_poorl3.loc[df_poorl3['ID']==subject_id,'L3slice'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment L3 Axial Images and Calculate Muscle Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_images = load_object(,os.path.join(pickles,'l3_images_cv_outliershandled.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compare_best_to_manual_l3_and_seg import seg_model_configs\n",
    "from compare_best_to_manual_l3_and_seg import do_segmentation_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/models/muscle/cv_final'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"muscle_segmentor\"]['model_path_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading l3 axial images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2352it [01:29, 26.31it/s]\n",
      "  1%|▏         | 35/2352 [00:00<00:06, 343.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Removing table\n",
      " - Taking care of images that have a rescale value of -1024, as opposed to -2048 for cannon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2352/2352 [00:03<00:00, 651.29it/s]\n",
      "  4%|▍         | 101/2352 [00:00<00:02, 1008.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - zeroing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2352/2352 [00:02<00:00, 866.02it/s] \n",
      " 17%|█▋        | 391/2352 [00:00<00:00, 3902.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Taking care of maximum value: default max is 4000 for zeroed images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2352/2352 [00:00<00:00, 7054.31it/s]\n",
      "  0%|          | 1/2352 [00:00<04:30,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Taking care of maximum value: default sigma above which to denoise is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2352/2352 [00:58<00:00, 40.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - removing table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2352it [00:19, 119.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Thresholding images\n",
      "- Normalizing images\n",
      "- Resizing images\n",
      " Segmenting using all model configs\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_0.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_1.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_2.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_3.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_4.h5'}\n",
      "Finding Average Masks\n",
      "Finding sma\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configs = seg_model_configs(config[\"muscle_segmentor\"]['model_path_dir'])\n",
    "    smas,average_masks,tableless_images = do_segmentation_cv(configs, l3_images)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imsave\n",
    "\n",
    "def output_sma_results(output_dir, l3_images, tableless_images, average_masks, smas):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    csv_filename = os.path.join(output_dir, \"areas-mm2_by_subject_id.csv\")\n",
    "    with open(csv_filename, \"w\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"subject_id\", \"area_mm2\", \"sagittal_series\", \"axial_series\"])\n",
    "        print('Saving Segmentation Results in ', output_dir)\n",
    "        index = 0    \n",
    "        for mask, sma, l3_image, tableless_image in zip(average_masks, smas,l3_images, tableless_images):\n",
    "            index += 1\n",
    "            base = os.path.join(output_dir, str(index) + \"_\" + l3_image.subject_id)\n",
    "            imsave(base + \"_CT.tif\", tableless_image.astype(np.float32))\n",
    "            imsave(base + \"_muscle.tif\", mask * np.iinfo(np.uint8).max)\n",
    "\n",
    "            row = [\n",
    "                l3_image.subject_id,\n",
    "                sma.area_mm2,\n",
    "                l3_image.sagittal_series.series_name,\n",
    "                l3_image.axial_series.series_name,\n",
    "            ]\n",
    "            csv_writer.writerow(row)\n",
    "        print('Total exams outputted: ', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Segmentation Results in  /tf/output/cv_poorl3/ms\n",
      "Total exams outputted:  2352\n"
     ]
    }
   ],
   "source": [
    "output_sma_results(config[\"muscle_segmentor\"]['output_directory'], l3_images, tableless_images, average_masks, smas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
