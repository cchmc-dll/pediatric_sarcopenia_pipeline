{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Segment Skeletal Muscle Area from MRI images (TIFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load libraries and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/smipipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/tf/smipipeline')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from IPython import get_ipython\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "import tables\n",
    "from MRI_notebooks.TIF_loader import mri_TIF_loader\n",
    "from unet3d.normalize import normalize_data_storage_2D\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    " \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# Custom functions\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):        \n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/smipipeline\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "data = '/tf/data'\n",
    "pickles = '/tf/pickles'\n",
    "models = '/tf/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all_modalities': ['MR'],\n",
      " 'data_file': '/tf/data/sarcopeniaMR_L3_h5/mri.h5',\n",
      " 'image_masks': ['truth'],\n",
      " 'input_type': 'Image',\n",
      " 'labels': ['1'],\n",
      " 'output_dir': '/tf/output/mri/',\n",
      " 'overwrite': False,\n",
      " 'prediction_model_path': '/tf/models/muscle/mri/ct_mri_invert_fresh.h5',\n",
      " 'problem_type': 'Segmentation',\n",
      " 'show_plots': False,\n",
      " 'testing_split': '/tf/pickles/mri/test_0.2.pkl',\n",
      " 'training_modalities': ['MR'],\n",
      " 'training_split': '/tf/pickles/mri/train_0.7.pkl',\n",
      " 'validation_split': '/tf/pickles/mri/validation_0.1.pkl'}\n"
     ]
    }
   ],
   "source": [
    "# Import modules and config file\n",
    "configfile = os.path.join(cwd,'config/mri/sma_mri_prediction_fresh.json')\n",
    "with open(configfile, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set overwrite to false when extracting imags from h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['overwrite'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = config['data_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening h5 file in Read mode...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check if Output file already exists, If it exists, require user permission to overwrite\n",
    "if 'overwrite' in config:\n",
    "    overwrite = config[\"overwrite\"]\n",
    "elif os.path.exists(output_file):\n",
    "    overwrite = input(\"Output file exists, do you want to overwrite? (y/n) \\n\")\n",
    "    overwrite = True if overwrite == 'y' else False   \n",
    "\n",
    "    # Open the hdf5 file\n",
    "if overwrite:\n",
    "    print(\"Opening h5 file in Write mode...\")\n",
    "    hdf5_file = tables.open_file(output_file, mode='w')\n",
    "else:\n",
    "    print(\"Opening h5 file in Read mode...\")\n",
    "    hdf5_file = tables.open_file(output_file, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of subjects in h5 file 201\n"
     ]
    }
   ],
   "source": [
    "subject_ids_final = hdf5_file.root.subject_ids[:]\n",
    "subject_ids_final = [subject.decode(\"utf-8\")for subject in subject_ids_final]\n",
    "print(\"Total number of subjects in h5 file\", len(subject_ids_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exam 109c', 'exam 97b', 'exam 1', 'exam 46', 'exam 107']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_ids_final[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file =hdf5_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = config['testing_split']\n",
    "pickle_list = load_object(pickle_file)\n",
    "pickle_name = pickle_file.split('/')[-1].split('.pkl')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['output_directory'] = os.path.join(config['output_dir'],pickle_name)\n",
    "if not os.path.exists(config['output_directory']):\n",
    "    os.makedirs(config['output_directory'])\n",
    "output_dir = config['output_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model_path'] = config['prediction_model_path']\n",
    "#config['model_path'] = config['transfer_learning_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_sma_experiment import configure_and_load_model\n",
    "model = configure_and_load_model(config[\"model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_modalities = config['training_modalities']\n",
    "config['threshold'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet3d.prediction2D import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "10\n",
      "65\n",
      "64\n",
      "100\n",
      "169\n",
      "82\n",
      "99\n",
      "147\n",
      "193\n",
      "154\n",
      "162\n",
      "3\n",
      "136\n",
      "143\n",
      "37\n",
      "80\n",
      "18\n",
      "56\n",
      "95\n",
      "7\n",
      "52\n",
      "36\n",
      "9\n",
      "98\n",
      "171\n",
      "125\n",
      "44\n",
      "93\n",
      "115\n",
      "189\n",
      "83\n",
      "8\n",
      "173\n",
      "21\n",
      "195\n",
      "192\n",
      "132\n",
      "191\n",
      "104\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "for index in pickle_list:\n",
    "    print(index)\n",
    "    if 'subject_ids' in data_file.root:\n",
    "        case_directory = os.path.join(output_dir, data_file.root.subject_ids[index].decode('utf-8'))\n",
    "    else:\n",
    "        case_directory = os.path.join(output_dir, \"validation_case_{}\".format(index))\n",
    "\n",
    "    run_validation_case(data_index=index, output_dir=case_directory, model=model, data_file=data_file,\n",
    "                                training_modalities=config[\"training_modalities\"], output_label_map=True, labels=config[\"labels\"],\n",
    "                                threshold=config[\"threshold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_evaluation_onTIF import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_predict = config.copy()\n",
    "config['input_dir'] = '/tf/output/mri/test_0.2'\n",
    "config['truth_img'] = 'truth'\n",
    "config['prediction_img'] = 'prediction'\n",
    "config['output_file'] = 'test_dsc_freshmodel.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of df: Index(['exam 46', 'exam 146', 'exam 120', 'exam 152b', 'exam 144b', 'exam 142',\n",
      "       'exam 119', 'exam 82', 'exam 35', 'exam 103', 'exam 118', 'exam 91',\n",
      "       'exam 69', 'exam 52', 'exam 80b', 'exam 186', 'exam 65', 'exam 115',\n",
      "       'exam 3', 'exam 27', 'exam 210', 'exam 202', 'exam 5', 'exam 180',\n",
      "       'exam 128', 'exam 51', 'exam 36', 'exam 29', 'exam 39', 'exam 156b',\n",
      "       'exam 30', 'exam 93', 'exam 84', 'exam 26', 'exam 130', 'exam 48',\n",
      "       'exam 85', 'exam 106', 'exam 20', 'exam 148b', 'exam 121'],\n",
      "      dtype='object') \t exam 46      0.920182\n",
      "exam 146     0.894771\n",
      "exam 120     0.937284\n",
      "exam 152b    0.924297\n",
      "exam 144b    0.856232\n",
      "exam 142     0.940682\n",
      "exam 119     0.946655\n",
      "exam 82      0.934533\n",
      "exam 35      0.934944\n",
      "exam 103     0.907533\n",
      "exam 118     0.921734\n",
      "exam 91      0.935496\n",
      "exam 69      0.837310\n",
      "exam 52      0.947126\n",
      "exam 80b     0.915924\n",
      "exam 186     0.908022\n",
      "exam 65      0.932910\n",
      "exam 115     0.931034\n",
      "exam 3       0.924910\n",
      "exam 27      0.914424\n",
      "exam 210     0.793219\n",
      "exam 202     0.860476\n",
      "exam 5       0.878833\n",
      "exam 180     0.681084\n",
      "exam 128     0.885030\n",
      "exam 51      0.949303\n",
      "exam 36      0.938144\n",
      "exam 29      0.919980\n",
      "exam 39      0.932895\n",
      "exam 156b    0.923352\n",
      "exam 30      0.870173\n",
      "exam 93      0.922864\n",
      "exam 84      0.914907\n",
      "exam 26      0.902021\n",
      "exam 130     0.912325\n",
      "exam 48      0.790923\n",
      "exam 85      0.887067\n",
      "exam 106     0.900844\n",
      "exam 20      0.943494\n",
      "exam 148b    0.874371\n",
      "exam 121     0.937438\n",
      "Name: DSC, dtype: float64 \n",
      " \n",
      "\n",
      "Mean:  0.9020669653178607\n",
      "Median:  0.9199804113614104\n",
      "Min:  0.6810841983852365\n",
      "Max:  0.9493029150823827\n"
     ]
    }
   ],
   "source": [
    "header = (\"DSC\",)\n",
    "masking_functions = (get_mask,)\n",
    "rows = list()\n",
    "subject_ids = list()\n",
    "for case_folder in glob.glob(config['input_dir'] + \"/*\"):\n",
    "    if not os.path.isdir(case_folder):\n",
    "        continue\n",
    "    subject_ids.append(os.path.basename(case_folder))\n",
    "    truth_file = os.path.join(case_folder, config['truth_img']+\".TIF\")\n",
    "    truth = io.imread(truth_file)\n",
    "    prediction_file = os.path.join(case_folder, config['prediction_img']+\".TIF\")\n",
    "    prediction = io.imread(prediction_file)\n",
    "    rows.append([dice_coefficient(func(truth), func(prediction))for func in masking_functions])\n",
    "\n",
    "df = pd.DataFrame.from_records(rows, columns=header, index=subject_ids)\n",
    "print('Index of df:', df.index,'\\t', df['DSC'], '\\n \\n')\n",
    "print('Mean: ', np.mean(df['DSC']))\n",
    "print('Median: ', np.median(df['DSC']))\n",
    "print('Min: ', np.min(df['DSC']))\n",
    "print('Max: ', np.max(df['DSC']))\n",
    "\n",
    "df.to_csv(config['input_dir'] + '/' + config['output_file'])\n",
    "\n",
    "scores = dict()\n",
    "for index, score in enumerate(df.columns):\n",
    "    values = df.values.T[index]\n",
    "    scores[score] = values[np.isnan(values) == False]\n",
    "\n",
    "plt.boxplot(list(scores.values()), labels=list(scores.keys()))\n",
    "plt.ylabel(\"Dice Coefficient\")\n",
    "plt.savefig(os.path.join(config['input_dir'], \"validation_scores_boxplot_test2.png\"))\n",
    "plt.close()\n",
    "\n",
    "if os.path.exists(\"./training.log\"):\n",
    "    training_df = pd.read_csv(\"./training.log\").set_index('epoch')\n",
    "\n",
    "    plt.plot(training_df['loss'].values, label='training loss')\n",
    "    plt.plot(training_df['val_loss'].values, label='validation loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xlim((0, len(training_df.index)))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig('loss_graph_test1_train2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
