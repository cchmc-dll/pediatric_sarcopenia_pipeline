{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load libraries and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from IPython import get_ipython\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "\n",
    "# Custom modules for debugging\n",
    "from SliceViewer import ImageSliceViewer3D, ImageSliceViewer3D_1view,ImageSliceViewer3D_2views\n",
    "from investigate import *\n",
    "\n",
    "#pd.set_option(\"display.max_rows\", 10)\n",
    "      \n",
    "import json\n",
    "from run_sma_experiment import find_l3_images,output_images\n",
    "import pprint\n",
    "from L3_finder import *\n",
    "\n",
    "# Custom functions\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):        \n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_line_magic('tb', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/smipipeline\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "data = '/tf/data'\n",
    "pickles = '/tf/pickles/v5_8pts'\n",
    "pickles_old = '/tf/pickles/'\n",
    "models = '/tf/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l3_finder': {'cache_dir': '/tf/_cache/',\n",
      "               'cache_intermediate_results': True,\n",
      "               'dicom_dir': '/tf/data',\n",
      "               'model_path_dir': '/tf/models/l3/cv_final',\n",
      "               'new_tim_dicom_dir_structure': True,\n",
      "               'output_directory': '/tf/output/v5_cv_poorl3/l3',\n",
      "               'overwrite': True,\n",
      "               'save_plots': True,\n",
      "               'show_plots': False},\n",
      " 'muscle_segmentor': {'model_path_dir': '/tf/models/muscle/cv_final',\n",
      "                      'output_directory': '/tf/output/v5_cv_poorl3/ms'}}\n"
     ]
    }
   ],
   "source": [
    "# Import modules and config file\n",
    "configfile = os.path.join(cwd,'config/debug_ES/v5_run_prediction_CV_poorl3.json')\n",
    "with open(configfile, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Process final images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Load each study into subject object\n",
    "<br>\n",
    "Subject object defined in L3finder.ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['l3_finder']['new_tim_dicom_dir_structure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding subjects\n",
      "Subjects found:  2375\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "print(\"Finding subjects\")\n",
    "\n",
    "subjects = list(\n",
    "    find_subjects(\n",
    "        config['l3_finder'][\"dicom_dir\"],\n",
    "        new_tim_dir_structure=config['l3_finder']['new_tim_dicom_dir_structure']\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Subjects found: ', len(subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section-3 - check if there are subjects with multiple folders (studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding series\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe14445018741a294933627dae52845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 289 ms, sys: 78 ms, total: 367 ms\n",
      "Wall time: 352 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Find series images\n",
    "print(\"Finding series\")\n",
    "series = list(flatten(tqdm((s.find_series() for s in subjects),total=len(subjects))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of series found:  3767\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of series found: \", len(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering series\n",
      "CPU times: user 739 ms, sys: 1.01 s, total: 1.75 s\n",
      "Wall time: 3.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sagittal_series, axial_series, excluded_series = separate_series(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of valid pats:  2375\n",
      "Length of sagittal series 1392\n",
      "Length of axial series 2375\n",
      "Length of excluded series 0\n",
      "Length of all series in dataset 3767\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of valid pats: \", len(subjects))\n",
    "print(\"Length of sagittal series\", len(sagittal_series))\n",
    "print(\"Length of axial series\", len(axial_series))\n",
    "print(\"Length of excluded series\", len(excluded_series))\n",
    "print(\"Length of all series in dataset\", len(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure each subject has at the max only 1 axial and 1 sagittal series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ax duplicates:  []\n",
      "Sag duplicates:  []\n"
     ]
    }
   ],
   "source": [
    "ax_ids = [ax.subject.id_ for ax in axial_series]\n",
    "sag_ids = [sag.subject.id_ for sag in sagittal_series]\n",
    "\n",
    "def find_duplicates(id_list):\n",
    "    uniques = []\n",
    "    duplicates = []\n",
    "    for ids in id_list:\n",
    "        if ids in uniques:\n",
    "            duplicates.append(ids)\n",
    "        else:\n",
    "            uniques.append(ids)\n",
    "            \n",
    "    return uniques,duplicates\n",
    "\n",
    "\n",
    "ax_u,ax_d = find_duplicates(ax_ids)\n",
    "sag_u,sag_d = find_duplicates(sag_ids)\n",
    "\n",
    "print('Ax duplicates: ', ax_d)\n",
    "print('Sag duplicates: ', sag_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the series objects to investigate\n",
    "ax_d_series = [ax for ax in axial_series if ax.subject.id_ in ax_d]\n",
    "sag_d_series = [ax for ax in sagittal_series if ax.subject.id_ in ax_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axials with duplicate:  0\n",
      "sagittals with duplicate:  0\n"
     ]
    }
   ],
   "source": [
    "print('axials with duplicate: ',len(ax_d_series))\n",
    "print('sagittals with duplicate: ',len(sag_d_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# df_dl= load_object(os.path.join(pickles_old,'df_final.pkl'))\n",
    "# display(df_dl[df_dl['ID']==ax_d[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct Missing Sagittals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTERING OUT 0.5 axials for recons for debugging!\n"
     ]
    }
   ],
   "source": [
    "# By default code filters 0.5mm slices, but I am letting them pass by setting it to 0\n",
    "constructed_sagittals = construct_series_for_subjects_without_sagittals(\n",
    "        subjects, sagittal_series, axial_series,thickness_filter=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series separated\n",
      " 1392 sagittal series. 2375 axial series. 0 excluded series. 983 constructed series.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "        \"Series separated\\n\",\n",
    "        len(sagittal_series), \"sagittal series.\",\n",
    "        len(axial_series), \"axial series.\",\n",
    "        len(excluded_series), \"excluded series.\",\n",
    "        len(constructed_sagittals), \"constructed series.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagittal_series.extend(constructed_sagittals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(axial_series,os.path.join(pickles,'axial_curated.pkl'))\n",
    "save_object(sagittal_series,os.path.join(pickles,'sagittal_curated.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sagittal MIPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2375/2375 [11:07<00:00,  3.56it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating sagittal MIPS\")\n",
    "mips = create_sagittal_mips_from_series(\n",
    "        many_series=sagittal_series,\n",
    "        cache_dir=config['l3_finder'].get(\"cache_dir\", None),\n",
    "        cache=config['l3_finder'].get(\"cache_intermediate_results\", False),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(mips,os.path.join(pickles,'mips.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mips = load_object(os.path.join(pickles,'mips.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "1it [00:00,  6.18it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "4it [00:00,  7.99it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "10it [00:00, 10.34it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "20it [00:00, 14.13it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "29it [00:00, 18.79it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "37it [00:00, 23.62it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "46it [00:00, 28.78it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "56it [00:01, 36.50it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "63it [00:01, 40.29it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "86it [00:01, 55.26it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "95it [00:01, 60.62it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "104it [00:01, 65.25it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "113it [00:01, 62.83it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "123it [00:01, 69.30it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "151it [00:02, 65.07it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "175it [00:02, 64.11it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "219it [00:03, 51.39it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "296it [00:04, 67.25it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "482it [00:07, 65.81it/s]/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "2375it [00:23, 102.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate heights for better batching\n",
      "Dimensions in set: dict_keys([(512, 512, 1), (1024, 512, 1)])\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing Images\")\n",
    "preprocessed_images = preprocess_images(mips)\n",
    "\n",
    "# Sagittal mip is redundant, get rid just use preprocessed images\n",
    "sagittal_mips = [SagittalMIP(i) for i in preprocessed_images]\n",
    "\n",
    "print(\"Separate heights for better batching\")\n",
    "mips_by_dimension = group_mips_by_dimension(sagittal_mips)\n",
    "print(\"Dimensions in set:\", mips_by_dimension.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(mips_by_dimension,os.path.join(pickles,'mips_by_dimension.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find L3 - step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mips_by_dimension = load_object(os.path.join(pickles,'mips_by_dimension.pkl'))\n",
    "axial_series = load_object(os.path.join(pickles,'axial_curated.pkl'))\n",
    "sagittal_series = load_object(os.path.join(pickles,'sagittal_curated.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNet1D_cv_1_of_5.h5', 'UNet1D_cv_2_of_5.h5', 'UNet1D_cv_3_of_5.h5', 'UNet1D_cv_4_of_5.h5', 'UNet1D_cv_5_of_5.h5']\n",
      "/tf/models/l3/cv_final/UNet1D_cv_1_of_5.h5\n",
      "/tf/models/l3/cv_final/UNet1D_cv_2_of_5.h5\n",
      "/tf/models/l3/cv_final/UNet1D_cv_3_of_5.h5\n",
      "/tf/models/l3/cv_final/UNet1D_cv_4_of_5.h5\n",
      "/tf/models/l3/cv_final/UNet1D_cv_5_of_5.h5\n"
     ]
    }
   ],
   "source": [
    "# Get all models in model path dir\n",
    "models_dir = config['l3_finder']['model_path_dir']\n",
    "\n",
    "# Get all models in models dir\n",
    "models_list = sorted([f for f in os.listdir(models_dir) if f.endswith('.h5')])\n",
    "print(models_list)\n",
    "folds = len(models_list)\n",
    "\n",
    "for fold in range(folds):\n",
    "    model_path = os.path.join(models_dir,models_list[fold])\n",
    "    print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for fold  0 Path:  /tf/models/l3/cv_final/UNet1D_cv_1_of_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error drawing line on preprocessed_image for: Z1996094\n",
      "shape: (125, 512, 1), prediction: 126\n",
      "\n",
      "error drawing line on preprocessed_image for: Z701477\n",
      "shape: (185, 512, 1), prediction: 186\n",
      "\n",
      "error drawing line on preprocessed_image for: Z495526\n",
      "shape: (215, 512, 1), prediction: 216\n",
      "\n",
      "error drawing line on preprocessed_image for: Z362374\n",
      "shape: (888, 512, 1), prediction: 888\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for fold  1 Path:  /tf/models/l3/cv_final/UNet1D_cv_2_of_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error drawing line on preprocessed_image for: Z892856\n",
      "shape: (430, 512, 1), prediction: 431\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1996094\n",
      "shape: (125, 512, 1), prediction: 127\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1140510\n",
      "shape: (245, 512, 1), prediction: 246\n",
      "\n",
      "error drawing line on preprocessed_image for: Z495526\n",
      "shape: (215, 512, 1), prediction: 216\n",
      "\n",
      "error drawing line on preprocessed_image for: Z878416\n",
      "shape: (462, 512, 1), prediction: 463\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1196674\n",
      "shape: (410, 512, 1), prediction: 411\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1273303\n",
      "shape: (495, 512, 1), prediction: 496\n",
      "\n",
      "error drawing line on preprocessed_image for: Z391986\n",
      "shape: (420, 512, 1), prediction: 421\n",
      "\n",
      "error drawing line on preprocessed_image for: Z526993\n",
      "shape: (282, 512, 1), prediction: 283\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1239639\n",
      "shape: (360, 512, 1), prediction: 361\n",
      "\n",
      "error drawing line on preprocessed_image for: Z913179\n",
      "shape: (220, 512, 1), prediction: 221\n",
      "\n",
      "error drawing line on preprocessed_image for: Z331988\n",
      "shape: (435, 512, 1), prediction: 437\n",
      "\n",
      "error drawing line on preprocessed_image for: Z854818\n",
      "shape: (425, 512, 1), prediction: 426\n",
      "\n",
      "error drawing line on preprocessed_image for: Z15992\n",
      "shape: (365, 512, 1), prediction: 367\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1293823\n",
      "shape: (185, 512, 1), prediction: 186\n",
      "\n",
      "error drawing line on preprocessed_image for: Z857466\n",
      "shape: (410, 512, 1), prediction: 411\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1302641\n",
      "shape: (752, 512, 1), prediction: 753\n",
      "\n",
      "error drawing line on preprocessed_image for: Z511402\n",
      "shape: (540, 512, 1), prediction: 541\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1211258\n",
      "shape: (576, 512, 1), prediction: 577\n",
      "\n",
      "error drawing line on preprocessed_image for: Z486791\n",
      "shape: (650, 512, 1), prediction: 651\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for fold  2 Path:  /tf/models/l3/cv_final/UNet1D_cv_3_of_5.h5\n",
      "Making predictions for fold  3 Path:  /tf/models/l3/cv_final/UNet1D_cv_4_of_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error drawing line on preprocessed_image for: Z495526\n",
      "shape: (215, 512, 1), prediction: 216\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for fold  4 Path:  /tf/models/l3/cv_final/UNet1D_cv_5_of_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error drawing line on preprocessed_image for: Z627604\n",
      "shape: (455, 512, 1), prediction: 458\n",
      "\n",
      "error drawing line on preprocessed_image for: Z1996094\n",
      "shape: (125, 512, 1), prediction: 127\n",
      "\n",
      "error drawing line on preprocessed_image for: Z495526\n",
      "shape: (215, 512, 1), prediction: 217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runname = 'CV_poorl3'\n",
    "if __name__ == \"__main__\":\n",
    "    for fold in range(folds):\n",
    "        model_path = os.path.join(models_dir,models_list[fold])\n",
    "        print(\"Making predictions for fold \", fold, 'Path: ', model_path)\n",
    "        prediction_results = []\n",
    "        prediction_errors = []\n",
    "        for dimension, sagittal_mips in mips_by_dimension.items():\n",
    "            dim_group_results,errors = make_predictions_for_sagittal_mips(\n",
    "                sagittal_mips,\n",
    "                model_path=model_path,\n",
    "                shape=dimension\n",
    "            )\n",
    "            prediction_results.extend(dim_group_results)\n",
    "            prediction_errors.extend(errors)\n",
    "\n",
    "        # Save prediction results\n",
    "        pred_results_file = 'prediction_results_' + str(fold) + '_' +  runname + '.pkl'\n",
    "        pred_errors_file = 'prediction_errors_' + str(fold) + '_' +  runname + '.pkl'\n",
    "        save_object(prediction_results,os.path.join(pickles,pred_results_file))\n",
    "        save_object(prediction_errors,os.path.join(pickles,pred_errors_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save l3 prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_series = load_object(os.path.join(pickles,'axial_curated.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2375"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(axial_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Total predictions:  2371\n",
      "Building L3 images for fold:  0\n",
      "Total images:  2371\n",
      "Outputting L3 images for fold:  0\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 950/2371 [02:33<10:49,  2.19it/s]  /tf/smipipeline/l3finder/output.py:142: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  assert img_max + (-img_min) < 65536\n",
      "100%|██████████| 2371/2371 [12:31<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions:  2355\n",
      "Building L3 images for fold:  1\n",
      "Total images:  2355\n",
      "Outputting L3 images for fold:  1\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2355/2355 [12:47<00:00,  3.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions:  2375\n",
      "Building L3 images for fold:  2\n",
      "Total images:  2375\n",
      "Outputting L3 images for fold:  2\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 1408/2375 [03:35<02:20,  6.90it/s] /tf/smipipeline/l3finder/output.py:142: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  assert img_max + (-img_min) < 65536\n",
      "100%|██████████| 2375/2375 [12:33<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions:  2374\n",
      "Building L3 images for fold:  3\n",
      "Total images:  2374\n",
      "Outputting L3 images for fold:  3\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 1395/2374 [03:34<02:40,  6.08it/s] /tf/smipipeline/l3finder/output.py:142: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  assert img_max + (-img_min) < 65536\n",
      " 96%|█████████▋| 2290/2374 [08:33<00:38,  2.21it/s]/tf/smipipeline/l3finder/output.py:142: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  assert img_max + (-img_min) < 65536\n",
      "100%|██████████| 2374/2374 [12:41<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions:  2372\n",
      "Building L3 images for fold:  4\n",
      "Total images:  2372\n",
      "Outputting L3 images for fold:  4\n",
      "Slow unless axial images already loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2372/2372 [12:38<00:00,  3.13it/s] \n"
     ]
    }
   ],
   "source": [
    "# Load prediction_results pickle files\n",
    "runname = 'CV_poorl3'\n",
    "\n",
    "output_dir = config[\"l3_finder\"][\"output_directory\"]\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "prediction_results_list = sorted([f for f in os.listdir(pickles) if (f.startswith('prediction_results_') and f.endswith(runname+'.pkl'))])\n",
    "folds = len(prediction_results_list)\n",
    "print(folds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold in range(folds):\n",
    "        pred_file = os.path.join(pickles,prediction_results_list[fold])\n",
    "        prediction_results = load_object(pred_file)\n",
    "        print('Total predictions: ',len(prediction_results))\n",
    "        print('Building L3 images for fold: ', fold)\n",
    "        l3_images = build_l3_images(axial_series, prediction_results)\n",
    "        print('Total images: ',len(l3_images))\n",
    "        # Don't run this unless you have new L3 results\n",
    "        print(\"Outputting L3 images for fold: \", fold)\n",
    "        # Clears pixel data from memory aafter outputting\n",
    "        output_dir = os.path.join(config[\"l3_finder\"][\"output_directory\"],str(fold))\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        l3_images = output_images(\n",
    "        l3_images,\n",
    "        args=dict(\n",
    "            output_directory=output_dir,\n",
    "            should_plot=config[\"l3_finder\"][\"show_plots\"],\n",
    "            should_overwrite=config[\"l3_finder\"][\"overwrite\"],\n",
    "            should_save_plots=config[\"l3_finder\"][\"save_plots\"]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find mean L3 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "from L3_finder import L3Image\n",
    "from l3finder import ingest\n",
    "\n",
    "from compare_best_to_manual_l3_and_seg import MinimalPrediction, MinimalResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_l3_predictions(l3_prediction_dir,nfolds):\n",
    "    subject_id_col = 0\n",
    "    pred_in_px_col = 1\n",
    "    predictions = defaultdict(list)\n",
    "\n",
    "    for fold_index in range(0, nfolds):\n",
    "        csv_dir = os.path.join(l3_prediction_dir,str(fold_index))\n",
    "        csv_path = Path(csv_dir,'l3_prediction_results.csv')\n",
    "        with open(csv_path) as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader)\n",
    "\n",
    "            for row in reader:\n",
    "                subid = row[subject_id_col].split('-')[0]\n",
    "                predictions[subid].append(float(row[pred_in_px_col]))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def calc_mean_predictions(all_predictions: defaultdict):\n",
    "    result = {}\n",
    "    for subject_id, prediction_list in all_predictions.items():\n",
    "        result[subject_id] = np.mean(prediction_list)\n",
    "    return result\n",
    "\n",
    "def find_subjects_w_preds(predictions, all_subjects):\n",
    "    subject_ids_w_preds = set(predictions.keys())\n",
    "    return [s for s in all_subjects if s.id_ in subject_ids_w_preds]\n",
    "\n",
    "def load_l3_images_from_predictions(mean_predictions, subjects_w_preds,axials,sagittals):\n",
    "    l3_images = []\n",
    "\n",
    "    for subject in subjects_w_preds:\n",
    "        sagittal_series = [s for s in sagittals if s.subject.id_ == subject.id_][0]\n",
    "        axial_series = [a for a in axials if a.subject.id_ == subject.id_][0]\n",
    "        l3_images.append(\n",
    "            L3Image(\n",
    "                axial_series=axial_series,\n",
    "                sagittal_series=sagittal_series,\n",
    "                prediction_result=MinimalResult(\n",
    "                    MinimalPrediction(\n",
    "                        predicted_y_in_px=mean_predictions[subject.id_]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    return l3_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_series = load_object(os.path.join(pickles,'axial_curated.pkl'))\n",
    "sagittal_series = load_object(os.path.join(pickles,'sagittal_curated.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "runname = 'CV_poorl3'\n",
    "prediction_results_list = sorted([f for f in os.listdir(pickles) if (f.startswith('prediction_results_') and f.endswith(runname+'.pkl'))])\n",
    "folds = len(prediction_results_list)\n",
    "print(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    all_predictions = load_l3_predictions(config[\"l3_finder\"][\"output_directory\"],folds)\n",
    "    mean_predictions = calc_mean_predictions(all_predictions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    subjects_w_preds = find_subjects_w_preds(mean_predictions, list(ingest.find_subjects(config['l3_finder'][\"dicom_dir\"])))\n",
    "    l3_images = load_l3_images_from_predictions(mean_predictions, subjects_w_preds, axial_series, sagittal_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(l3_images,os.path.join(pickles,'l3_images_cv.pkl'))\n",
    "save_object(mean_predictions,os.path.join(pickles,'mean_predictions.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Outlier Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_images = load_object(os.path.join(pickles,'l3_images_cv.pkl'))\n",
    "mean_predictions = load_object(os.path.join(pickles,'mean_predictions.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outliers for manual L3 detection:  45\n",
      "Cases with L3 not present:  15\n",
      "Cases with manually identified L3s:  30\n",
      "Outliers:  45\n"
     ]
    }
   ],
   "source": [
    "infile  = 'poorl3.csv'\n",
    "df_poorl3 = pd.read_csv(infile, index_col=False)\n",
    "\n",
    "print('Total number of outliers for manual L3 detection: ', len(df_poorl3))\n",
    "l3_absent = df_poorl3.loc[df_poorl3['L3slice'].isnull(),'ID'].values.tolist()\n",
    "print('Cases with L3 not present: ', len(l3_absent))\n",
    "l3_present = df_poorl3.loc[~df_poorl3['L3slice'].isnull(),'ID'].values.tolist()\n",
    "print('Cases with manually identified L3s: ', len(l3_present))\n",
    "\n",
    "l3_outliers = l3_absent + l3_present\n",
    "print(\"Outliers: \", len(l3_outliers))\n",
    "\n",
    "#sagittal_mips_valid = [sagittal_mip for sagittal_mip in sagittal_mips if sagittal_mip.subject_id not in df_poorl3.ID.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total l3_images:  2375\n",
      "Total l3_images after outlier removal:  2360\n"
     ]
    }
   ],
   "source": [
    "# Get rid of outliers without proper L3 images\n",
    "print('Total l3_images: ', len(l3_images))\n",
    "l3_images = [l3_image for l3_image in l3_images if l3_image.subject_id not in l3_absent]\n",
    "print('Total l3_images after outlier removal: ', len(l3_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "l3_images_out = [l3_image for l3_image in l3_images if l3_image.subject_id in l3_present]\n",
    "print(len(l3_images_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330\n"
     ]
    }
   ],
   "source": [
    "l3_images_normals = [l3_image for l3_image in l3_images if l3_image.subject_id not in l3_outliers]\n",
    "print(len(l3_images_normals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Manual Predictions DICT\n",
    "manualL3s = []\n",
    "for i in range(len(l3_images_out)):\n",
    "    subject_id = l3_images_out[i].subject_id\n",
    "    manualL3s.append(df_poorl3.loc[df_poorl3['ID']==subject_id,'L3slice'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(l3_images_normals,os.path.join(pickles,'l3_images_normals.pkl'))\n",
    "save_object(l3_images_out,os.path.join(pickles,'l3_images_outliers.pkl'))\n",
    "save_object(manualL3s,os.path.join(pickles,'manualL3s.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment L3 Axial Images and Calculate Muscle Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_images_normals = load_object(os.path.join(pickles,'l3_images_normals.pkl'))\n",
    "l3_images_out = load_object(os.path.join(pickles,'l3_images_outliers.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2238\n"
     ]
    }
   ],
   "source": [
    "# List from epic filter \n",
    "# Changed for V5 to read from csv file\n",
    "df_v5 = pd.read_csv('patlist_with_validBMI_corrected_v5.csv', index_col=False)\n",
    "normal_patients_corrected = list(df_v5.PAT_ID.values)\n",
    "print(len(normal_patients_corrected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df_l3_present:  30\n",
      "Length of df_l3_present_normals:  27\n"
     ]
    }
   ],
   "source": [
    "# List from Andrew\n",
    "infile  = 'poorl3.csv'\n",
    "df_poorl3 = pd.read_csv(infile, index_col=False)\n",
    "\n",
    "df_l3_present = df_poorl3.loc[~df_poorl3['L3slice'].isnull()]\n",
    "\n",
    "print('Length of df_l3_present: ', len(df_l3_present))\n",
    "\n",
    "df_l3_present_normals = df_l3_present.loc[df_l3_present['ID'].isin(normal_patients_corrected)]\n",
    "\n",
    "print('Length of df_l3_present_normals: ', len(df_l3_present_normals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of normals processed and in epic filter:  2170\n",
      "Length of outliers processed and in epic filter:  27\n",
      "Length of manual L3s:  27\n"
     ]
    }
   ],
   "source": [
    "# Process only the normals selected by epic filter\n",
    "l3_images_normals = [l3 for l3 in l3_images_normals if l3.subject_id in normal_patients_corrected]\n",
    "print('Length of normals processed and in epic filter: ', len(l3_images_normals))\n",
    "\n",
    "l3_images_out = [l3 for l3 in l3_images_out if l3.subject_id in normal_patients_corrected]\n",
    "print('Length of outliers processed and in epic filter: ', len(l3_images_out))\n",
    "\n",
    "manualL3s = [int(df_l3_present_normals.loc[df_l3_present_normals['ID']==l3.subject_id,'L3slice'].values[0]) for l3 in l3_images_out if l3.subject_id]\n",
    "print('Length of manual L3s: ', len(manualL3s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients with L3:  2197\n",
      "Patients from Epic:  2238\n",
      "Patients Missing Axial CT:  41\n"
     ]
    }
   ],
   "source": [
    "# Patients in Epic filter, but not in normals or outliers [i.e those missing Axial CT itself]\n",
    "\n",
    "all_l3s = l3_images_normals + l3_images_out\n",
    "\n",
    "l3_pats = [l3.subject_id for l3 in all_l3s]\n",
    "\n",
    "missing_CT = [p for p in normal_patients_corrected if p not in l3_pats]\n",
    "\n",
    "print('Patients with L3: ', len(l3_pats))\n",
    "print('Patients from Epic: ',len(normal_patients_corrected))\n",
    "print('Patients Missing Axial CT: ', len(missing_CT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compare_best_to_manual_l3_and_seg import seg_model_configs\n",
    "from compare_best_to_manual_l3_and_seg import do_segmentation_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/models/muscle/cv_final'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"muscle_segmentor\"]['model_path_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading l3 axial images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2170it [01:16, 28.49it/s]\n",
      "  1%|▏         | 31/2170 [00:00<00:07, 304.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Removing table\n",
      " - Taking care of images that have a rescale value of -1024, as opposed to -2048 for cannon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2170/2170 [00:03<00:00, 586.32it/s]\n",
      "  7%|▋         | 157/2170 [00:00<00:01, 1568.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - zeroing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2170/2170 [00:01<00:00, 1539.73it/s]\n",
      "100%|██████████| 2170/2170 [00:00<00:00, 13208.05it/s]\n",
      "  0%|          | 0/2170 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Taking care of maximum value: default max is 4000 for zeroed images\n",
      " - Taking care of maximum value: default sigma above which to denoise is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2170/2170 [00:45<00:00, 48.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - removing table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2170it [00:18, 114.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Thresholding images\n",
      "- Normalizing images\n",
      "- Resizing images\n",
      " Segmenting using all model configs\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_0.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_1.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_2.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_3.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_4.h5'}\n",
      "Finding Average Masks\n",
      "Finding sma\n",
      "Length of smas normals:  2170\n",
      "Length of average_masks normals:  2170\n",
      "Length of tableless_images normals:  2170\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configs = seg_model_configs(config[\"muscle_segmentor\"]['model_path_dir'])\n",
    "    smas,average_masks,tableless_images = do_segmentation_cv(configs, l3_images_normals)\n",
    "    print('Length of smas normals: ',len(smas))\n",
    "    print('Length of average_masks normals: ',len(average_masks))\n",
    "    print('Length of tableless_images normals: ',len(tableless_images))\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manualL3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading l3 axial images\n",
      "Manual L3 locations found, using those to segment!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:11,  2.29it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 343.25it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 766.33it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 7518.17it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Removing table\n",
      " - Taking care of images that have a rescale value of -1024, as opposed to -2048 for cannon images\n",
      "  - zeroing images\n",
      " - Taking care of maximum value: default max is 4000 for zeroed images\n",
      " - Taking care of maximum value: default sigma above which to denoise is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 40.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - removing table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "27it [00:00, 49.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Thresholding images\n",
      "- Normalizing images\n",
      "- Resizing images\n",
      " Segmenting using all model configs\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_0.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_1.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_2.h5'}\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_3.h5'}\n",
      "WARNING:tensorflow:5 out of the last 72 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5094311ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "segmenting for model {'model_path': '/tf/models/muscle/cv_final/combined_2020-02-18_dice_fold_4.h5'}\n",
      "WARNING:tensorflow:6 out of the last 73 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f508c5cf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Finding Average Masks\n",
      "Finding sma\n",
      "Length of smas outliers:  27\n",
      "Length of average_masks outliers:  27\n",
      "Length of tableless_images outliers:  27\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configs = seg_model_configs(config[\"muscle_segmentor\"]['model_path_dir'])\n",
    "    smas_out,average_masks_out,tableless_images_out = do_segmentation_cv(configs, l3_images_out, manualL3s)\n",
    "    print('Length of smas outliers: ',len(smas_out))\n",
    "    print('Length of average_masks outliers: ',len(average_masks_out))\n",
    "    print('Length of tableless_images outliers: ',len(tableless_images_out))\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smas = smas + smas_out\n",
    "average_masks = average_masks + average_masks_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableless_images = np.concatenate((tableless_images, tableless_images_out),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_images = l3_images_normals + l3_images_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of smas all:  2197\n",
      "Length of average_masks all:  2197\n",
      "Length of tableless_images all:  2197\n"
     ]
    }
   ],
   "source": [
    "print('Length of smas all: ',len(smas))\n",
    "print('Length of average_masks all: ',len(average_masks))\n",
    "print('Length of tableless_images all: ',len(tableless_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imsave\n",
    "import csv\n",
    "\n",
    "def output_sma_results(output_dir, l3_images, tableless_images, average_masks, smas):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    csv_filename = os.path.join(output_dir, \"areas-mm2_by_subject_id.csv\")\n",
    "    with open(csv_filename, \"w\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"subject_id\", \"area_mm2\", \"sagittal_series\", \"axial_series\"])\n",
    "        print('Saving Segmentation Results in ', output_dir)\n",
    "        index = 0    \n",
    "        for mask, sma, l3_image, tableless_image in zip(average_masks, smas,l3_images, tableless_images):\n",
    "            index += 1\n",
    "            base = os.path.join(output_dir, str(index) + \"_\" + l3_image.subject_id)\n",
    "            imsave(base + \"_CT.tif\", tableless_image.astype(np.float32))\n",
    "            imsave(base + \"_muscle.tif\", mask * np.iinfo(np.uint8).max)\n",
    "\n",
    "            row = [\n",
    "                l3_image.subject_id,\n",
    "                sma.area_mm2,\n",
    "                l3_image.sagittal_series.series_name,\n",
    "                l3_image.axial_series.series_name,\n",
    "            ]\n",
    "            csv_writer.writerow(row)\n",
    "        print('Total exams outputted: ', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Segmentation Results in  /tf/output/v5_cv_poorl3/ms\n",
      "Total exams outputted:  2197\n"
     ]
    }
   ],
   "source": [
    "output_sma_results(config[\"muscle_segmentor\"]['output_directory'], l3_images, tableless_images, average_masks, smas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
